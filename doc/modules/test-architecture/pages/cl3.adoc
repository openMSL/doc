# CL 3: Quantitative Verification

For credibility assessment level 3, the model is again employed in a co-simulation with a trace file player or scenario engine.
During the simulation, a trace file is generated containing the model output of every simulation time step.
This is done by connecting a https://github.com/Persival-GmbH/osi-sensordata-trace-file-writer[trace file writer] to the model output in the SSP specification of the test.
The trace file writer is pre-built in every test environment.
The FMU is located at _/tmp/tracefile_writer_fmu/osi-sensordata-trace-file-writer.fmu_.
After the co-simulation is finished, the GitHub action scans the test folder for python files.
If it finds one, the python script is called with the path to the just generated trace file as an argument.
The file name of the python file does not matter.
However, if there is more than one python file in the directory, the analysis will not be performed since it is unclear, which file to use.
If supplementary files are necessary for the analysis, they are to be placed in a subfolder.

The purpose is to test the proper implementation of the model code in terms of its quantifiable requirements.
E.g. the sensor is rotated around all 3 axis, and it is evaluated, if the coordinate transformations in the model are correct.
This can be achieved in two ways:

1. The simulation trace file is analyzed by a custom standalone python script.
2. The simulation trace file is compared to an expectation trace file with a python script.

The general setup of this test level is depicted in the following image and both approaches are described in more detail in the following sections.

.Test setup for credibility assessment level 3
[#img-cl3_test,link=_images/cl3_test.svg]
image::cl3_test.svg[Test setup for credibility assessment level 3,600]

## CL 3.1 Standalone Trace File Analysis

The OSI trace file generated by the co-simulation is read in by a python script after the simulation has finished.
The python script can perform a variety of tests on the trace file, depending on the specific scenario and use-case.
For example, the scenario places an object at a certain distance inside the field of view of a sensor.
The python script can analyze the sensor model output and check, if the object was detected at the expected distance.
Another example would be to place an object outside of the field of view and check, that it was not detected.
Furthermore, distribution functions for detections or expected number of detections on an object can be checked.

An example implementation of this test can be found in the https://github.com/openMSL/sl-1-0-sensor-model-repository-template/tree/main/test/integration/004_tracefile_analysis[sensor model template repository].

## CL 3.2 Trace File Comparison

(not yet implemented) +
This test is similar to 3.1.
But instead of analyzing the simulation trace file with certain criteria, the python script compares the model output to an expectation trace files.
The expectation trace file can either contain other simulated data or even measurement data from a real system to compare the model under test to.
This comparison is done with appropriate metrics.
Required thresholds for these metrics determine the pass/fail criterion of these tests.